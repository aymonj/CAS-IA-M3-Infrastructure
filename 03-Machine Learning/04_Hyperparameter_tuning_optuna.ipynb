{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf89f6c4-d472-4057-bd0a-61c58754b249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Pipeline using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d680960-9bf6-4f64-be2d-793a5d99e0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG jeromeaymon_lakehouse\")\n",
    "spark.sql(\"USE SCHEMA ml_sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75d1f88-8682-4584-b42f-08a91d1c40c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Volumes/jeromeaymon_lakehouse/ml_sandbox/data/train.csv\"\n",
    "train_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Cast Boolean columns to int\n",
    "train_df = train_df.withColumn(\"PassengerId\", col(\"PassengerId\").cast(\"string\")) \\\n",
    "                   .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                   .withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Transported\", col(\"Transported\").cast(\"int\")) \n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0482d56-3631-409f-acb6-494b6f227346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pandas & scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27ed428-55cb-46ef-b915-c522eb99334d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = train_df.toPandas()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d302f6b0-c7f7-4a0b-b93a-40d8b451b782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Define transformers for different column types\n",
    "numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\"))]\n",
    ")\n",
    "\n",
    "categorical_cols = ['HomePlanet', 'Destination', 'VIP', 'CryoSleep']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Step 2: Create a ColumnTransformer that applies the transformations to the columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# Step 3: Assemble the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(train)\n",
    "\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe53945-d4f5-4c3a-8e0d-f4d194869005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting back to Pandas DataFrame\n",
    "onehot_encoder_feature_names = list(preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out())\n",
    "column_order =  numerical_cols + onehot_encoder_feature_names\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=column_order, index=train.index)\n",
    "y = train['Transported']\n",
    "\n",
    "X_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139464e2-21cb-4c7f-aae6-58d46ec5a767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter tuning of a Decision Tree Classifier \n",
    "\n",
    "We use optuna to hyperparameter tuning of a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6945428-2cb3-4ce4-9c5e-5dc1d46f4b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', random_state= 42)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        # trial parameters to optimize\n",
    "        'max_depth' : trial.suggest_int('max_depth', 3, 40, log=True),\n",
    "        'min_samples_split' : trial.suggest_float('min_samples_split', 1e-6, 1e-3, log=True),\n",
    "        'min_samples_leaf' : trial.suggest_float('min_samples_leaf', 1e-6, 1e-3, log=True)\n",
    "    }\n",
    "\n",
    "    model.set_params(**params)\n",
    "\n",
    "    cv_score = cross_val_score(model, X_preprocessed, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    return cv_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"best_params =\", study.best_params, \"with cross_validation_score =\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f74f4a-3e61-4392-8fb3-cea4a34305cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c864b400-ab60-4c57-b252-e0932e89fc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set the tracking URI to the Databricks workspace\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Create an instance of MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "X = train.drop(['Transported'], axis = 1)\n",
    "y = train['Transported']\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Fit the model with the best hyperparameters from the study\n",
    "    model = DecisionTreeClassifier(criterion= 'entropy', random_state= 42)\n",
    "    model.set_params(**study.best_params)\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "    \n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(study.best_params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", study.best_value)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Simple Decision Tree Classifier\")\n",
    "    \n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X, model_pipeline.predict(X))\n",
    "    \n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model_pipeline,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"decision_tree_model\",\n",
    "        artifact_path=\"decision_tree_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c94f634-cffa-472a-824d-fe0649c5957f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55241f96-cae3-47ab-8ada-e8c56953c32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Optuna: Distributed Hyperparameter Tuning\n",
    "\n",
    "We'll now use Optuna with **distributed parallelization** across Spark executors to accelerate hyperparameter tuning.\n",
    "\n",
    "**Key Features:**\n",
    "* **MlflowStorage**: Uses MLflow Tracking Server as the storage backend for sharing trial results\n",
    "* **MlflowSparkStudy**: Distributes trials across Spark executors for parallel execution\n",
    "* **n_jobs parameter**: Controls the number of parallel trials to run simultaneously\n",
    "\n",
    "This approach is ideal for computationally expensive models and large search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c064816-3193-477b-9693-dc4206cad22e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow>=3.0.0 optuna==4.1.0 --upgrade --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f1e80d-32e3-4bc1-801a-dd0138393646",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restore Variables After Kernel Restart"
    }
   },
   "outputs": [],
   "source": [
    "# Re-run necessary setup after kernel restart\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Set catalog and schema\n",
    "spark.sql(\"USE CATALOG jeromeaymon_lakehouse\")\n",
    "spark.sql(\"USE SCHEMA ml_sandbox\")\n",
    "\n",
    "# Load and prepare data\n",
    "data_path = \"/Volumes/jeromeaymon_lakehouse/ml_sandbox/data/train.csv\"\n",
    "train_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "train_df = train_df.withColumn(\"PassengerId\", col(\"PassengerId\").cast(\"string\")) \\\n",
    "                   .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                   .withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Transported\", col(\"Transported\").cast(\"int\"))\n",
    "train = train_df.toPandas()\n",
    "\n",
    "# Recreate preprocessor\n",
    "numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[('imputer', SimpleImputer(strategy=\"mean\"))]\n",
    ")\n",
    "\n",
    "categorical_cols = ['HomePlanet', 'Destination', 'VIP', 'CryoSleep']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[('encoder', OneHotEncoder())]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Variables restored: train, preprocessor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc349519-6725-4494-a689-32d9454677cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup MLflow Storage for Distributed Optimization"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.optuna.storage import MlflowStorage\n",
    "from mlflow.pyspark.optuna.study import MlflowSparkStudy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna\n",
    "\n",
    "# Get the current experiment ID\n",
    "experiment_id = mlflow.get_experiment_by_name(\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    ").experiment_id\n",
    "\n",
    "# Create MLflow storage backend for distributed optimization\n",
    "mlflow_storage = MlflowStorage(experiment_id=experiment_id)\n",
    "\n",
    "print(f\"Using experiment ID: {experiment_id}\")\n",
    "print(f\"MLflow storage configured for distributed optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9440748-3ce5-4fb0-b516-1178058ebfed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Objective Function with MLflow Tracking"
    }
   },
   "outputs": [],
   "source": [
    "def distributed_objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for distributed Optuna optimization.\n",
    "    Each trial is logged to MLflow automatically via MlflowStorage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40, log=True),\n",
    "        'min_samples_split': trial.suggest_float('min_samples_split', 1e-6, 1e-3, log=True),\n",
    "        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 1e-6, 1e-3, log=True),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.5),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1)\n",
    "    }\n",
    "    \n",
    "    # Create model with suggested parameters\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        random_state=42,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with preprocessing\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Prepare data\n",
    "    X = train.drop(['Transported'], axis=1)\n",
    "    y = train['Transported']\n",
    "    \n",
    "    # Evaluate with cross-validation\n",
    "    cv_score = cross_val_score(\n",
    "        model_pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1  # Use all available cores for CV\n",
    "    ).mean()\n",
    "    \n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5007664215656845,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa0415c-b722-404b-a93c-72b8d8812e69",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Distributed Optimization"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create Optuna study with MLflow storage backend\n",
    "# Note: Using standard Optuna study instead of MlflowSparkStudy because\n",
    "# MlflowSparkStudy requires a multi-node cluster with workers.\n",
    "# This approach still logs to MLflow and parallelizes using threads on the driver.\n",
    "# Use unique study name to avoid conflicts with existing studies\n",
    "study = optuna.create_study(\n",
    "    study_name=f\"decision_tree_tuning_{int(time.time())}\",\n",
    "    storage=mlflow_storage,\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=False\n",
    ")\n",
    "\n",
    "# Run optimization with thread-based parallelization\n",
    "# n_trials: total number of trials to run\n",
    "# n_jobs: number of parallel threads (runs on driver node)\n",
    "print(\"Starting hyperparameter optimization with MLflow tracking...\")\n",
    "print(f\"Total trials: 200\")\n",
    "print(f\"Parallel threads: 4\")\n",
    "print(\"\\nThis will parallelize using threads on the driver node.\\n\")\n",
    "\n",
    "study.optimize(\n",
    "    distributed_objective,\n",
    "    n_trials=200,\n",
    "    n_jobs=4,  # Run 4 trials in parallel using threads\n",
    "    catch=(Exception,)  # Catch exceptions during parallel execution to prevent None values\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Optimization Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best cross-validation score: {study.best_value:.4f}\")\n",
    "print(f\"Total trials completed: {len(study.trials)}\")\n",
    "print(f\"Failed trials: {len([t for t in study.trials if t.state != optuna.trial.TrialState.COMPLETE])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5007664215656845,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f0e2d9-25c3-420c-b892-713e8364c4ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyze Optimization Results"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get all trials as DataFrame\n",
    "trials_df = study.trials_dataframe()\n",
    "\n",
    "# Display top 10 trials\n",
    "print(\"Top 10 Trials by Accuracy:\")\n",
    "top_trials = trials_df.nlargest(10, 'value')[[\n",
    "    'number', 'value', 'params_max_depth', 'params_min_samples_split', \n",
    "    'params_min_samples_leaf', 'params_max_features'\n",
    "]]\n",
    "print(top_trials.to_string(index=False))\n",
    "\n",
    "# Visualize optimization history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Optimization history\n",
    "axes[0].plot(trials_df['number'], trials_df['value'], alpha=0.6, marker='o', markersize=3)\n",
    "axes[0].axhline(y=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.4f}')\n",
    "axes[0].set_xlabel('Trial Number')\n",
    "axes[0].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[0].set_title('Optimization History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter importance (max_depth)\n",
    "axes[1].scatter(trials_df['params_max_depth'], trials_df['value'], alpha=0.5)\n",
    "axes[1].set_xlabel('max_depth')\n",
    "axes[1].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[1].set_title('Impact of max_depth on Performance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5007664215656845,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8ba8b9-0603-4f2f-9008-c2f65a97c631",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train and Log Final Model with Best Parameters"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Prepare data\n",
    "X = train.drop(['Transported'], axis=1)\n",
    "y = train['Transported']\n",
    "\n",
    "# Start MLflow run for final model\n",
    "with mlflow.start_run(run_name=\"optimized_decision_tree\"):\n",
    "    \n",
    "    # Create model with best parameters from optimization\n",
    "    best_model = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        random_state=42,\n",
    "        **study.best_params\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', best_model)\n",
    "    ])\n",
    "    \n",
    "    # Train on full dataset\n",
    "    final_pipeline.fit(X, y)\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_param(\"optimization_method\", \"optuna_with_mlflow_storage\")\n",
    "    mlflow.log_param(\"total_trials\", len(study.trials))\n",
    "    mlflow.log_param(\"parallel_jobs\", 4)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"cv_accuracy\", study.best_value)\n",
    "    \n",
    "    # Set tags\n",
    "    mlflow.set_tag(\"Training Info\", \"Decision Tree with Optuna Optimization\")\n",
    "    mlflow.set_tag(\"optimization_type\", \"optuna_mlflow_storage\")\n",
    "    \n",
    "    # Infer signature\n",
    "    signature = infer_signature(X, final_pipeline.predict(X))\n",
    "    \n",
    "    # Log model to Unity Catalog\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=final_pipeline,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"decision_tree_optimized\",\n",
    "        artifact_path=\"decision_tree_model\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Final Model Logged Successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model URI: {model_info.model_uri}\")\n",
    "    print(f\"Registered Model: decision_tree_optimized\")\n",
    "    print(f\"Best CV Accuracy: {study.best_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Hyperparameter_tuning_optuna",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
